{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xAXNn1v3t69X",
        "outputId": "03e13fe8-ab9e-4431-8b61-f1e5f52df23a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2qtPrdqt69Z"
      },
      "source": [
        "### Reading train dataset & tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_UqVo9IXt69a",
        "outputId": "9f274e10-d089-4bd4-d970-361b0e801084",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from JPype1>=0.7.0->konlpy) (26.0)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (495 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.9/495.9 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.6.0 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3uf42BnSt69a"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Hannanum\n",
        "morph = Hannanum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lKppWRWQt69a",
        "outputId": "c377f56f-9594-4194-9fe4-0824fccc05ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kor.txt...\n",
            "kor.txt downloaded.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "# Download data files if not exist\n",
        "data_urls = {\n",
        "    'kor.txt': 'https://github.com/nongaussian/class-2026-lginnotek-llm/raw/refs/heads/main/nmt/kor.txt'\n",
        "}\n",
        "\n",
        "for filename, url in data_urls.items():\n",
        "    if not os.path.exists(filename):\n",
        "        print(f'Downloading {filename}...')\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "        print(f'{filename} downloaded.')\n",
        "    else:\n",
        "        print(f'{filename} already exists.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wdUu4k-Bt69b"
      },
      "outputs": [],
      "source": [
        "with open('kor.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "targ, inp = zip(*[line.split('\\t') for line in lines])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UEeLOvv5t69b",
        "outputId": "8df3a21a-defc-4c0f-ff52-cd853f8479b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결혼하거나 그 반대의 상황이 존재하지. 그런데 인간이 수백 명의 사람만 알고 지내는 사이가 될 기회를 갖는다고 생각해 보면, 또 그 수백 명 중 열여 명 쯤 이하만 잘 알 수 있고, 그리고 나서 그 열여 명 중에 한두 명만 친구가 될 수 있다면, 그리고 또 만일 우리가 이 세상에 살고 있는 수백만 명의 사람들만 기억하고 있다면, 딱 맞는 남자는 지구가 생겨난 이래로 딱 맞는 여자를 단 한번도 만난 적이 없을 수도 있을 거라는 사실을 쉽게 눈치챌 수 있을 거야.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "inp[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wny2C9qit69b",
        "outputId": "1c607418-d18a-4cd5-9b46-d85de110788c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['의심', '의', '여', '이', '지', '없이', '세상', '에는', '어떤', '남자', '이', '든', '정확히', '딱', '알맞', '는', '여자', '와', '결혼', '하', '거나', '그', '반대', '의', '상황', '이', '존재', '하', '지', '.', '그런데', '인간', '이', '수백', '명', '의', '사람', '만', '알', '고', '지내', '는', '사이', '가', '되', 'ㄹ', '기회', '를', '갖', '는다', '고', '생각', '하', '어', '보', '면', ',', '또', '그', '수백', '명', '중', '열', '여', '명', '쯤', '이하', '만', '잘', '알', 'ㄹ', '수', '있', '고', ',', '그리고', '나', '서', '그', '열', '여', '명', '중', '에', '한두', '명', '만', '친구', '가', '되', 'ㄹ', '수', '있', '다면', ',', '그리고', '또', '만', '이', 'ㄹ', '우리', '가', '이', '세상', '에', '살', '고', '있', '는', '수백만', '명', '의', '사람들', '만', '기억', '하고', '있', '다면', ',', '딱', '맞', '는', '남자', '는', '지구', '가', '생기', '어', '나', 'ㄴ', '이래', '로', '딱', '맞', '는', '여자', '를', '달', 'ㄴ', '한번', '도', '만나', 'ㄴ', '적', '이', '없', '을', '수', '도', '있', '을', '것', '이', '라는', '사실', '을', '쉽', '게', '눈치채', 'ㄹ', '수', '있', '을', '것', '이', '야', '.']\n"
          ]
        }
      ],
      "source": [
        "print(morph.morphs(inp[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5biWpMyot69b",
        "outputId": "2cec96ec-ba1c-4455-89ea-bba203166aab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'doubtless there exists in this world precisely the right woman for any given man to marry and vice versa; but when you consider that a human being has the opportunity of being acquainted with only a few hundred people, and out of the few hundred that there are but a dozen or less whom he knows intimately, and out of the dozen, one or two friends at most, it will easily be seen, when we remember the number of millions who inhabit this world, that probably, since the earth was created, the right man has never yet met the right woman.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "targ[-1].lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LcoR7qdyt69b",
        "outputId": "8589ccb6-5dcc-4cec-e7c3-6adfde8e45e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IybSWrGit69b",
        "outputId": "1fea77c1-39ce-4e73-e6a3-e59f4c419277",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['doubtless', 'there', 'exists', 'in', 'this', 'world', 'precisely', 'the', 'right', 'woman', 'for', 'any', 'given', 'man', 'to', 'marry', 'and', 'vice', 'versa', ';', 'but', 'when', 'you', 'consider', 'that', 'a', 'human', 'being', 'has', 'the', 'opportunity', 'of', 'being', 'acquainted', 'with', 'only', 'a', 'few', 'hundred', 'people', ',', 'and', 'out', 'of', 'the', 'few', 'hundred', 'that', 'there', 'are', 'but', 'a', 'dozen', 'or', 'less', 'whom', 'he', 'knows', 'intimately', ',', 'and', 'out', 'of', 'the', 'dozen', ',', 'one', 'or', 'two', 'friends', 'at', 'most', ',', 'it', 'will', 'easily', 'be', 'seen', ',', 'when', 'we', 'remember', 'the', 'number', 'of', 'millions', 'who', 'inhabit', 'this', 'world', ',', 'that', 'probably', ',', 'since', 'the', 'earth', 'was', 'created', ',', 'the', 'right', 'man', 'has', 'never', 'yet', 'met', 'the', 'right', 'woman', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "print(nltk.word_tokenize(targ[-1].lower()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uhzI-Glbt69b"
      },
      "outputs": [],
      "source": [
        "x_tokens = [ morph.morphs(x) for x in inp ]\n",
        "y_tokens = [ nltk.word_tokenize(x.lower()) for x in targ ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vQyGFQ_6t69c",
        "outputId": "179d3738-f820-4f31-cf38-0690d0796460",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3729 3729\n"
          ]
        }
      ],
      "source": [
        "print(len(x_tokens), len(y_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9VBPI9St69c"
      },
      "source": [
        "### Encoding text to numeric sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Hl3qLPW5t69c"
      },
      "outputs": [],
      "source": [
        "def text_encoding(lines):\n",
        "    vocab, index = {}, 3  # start indexing from 3\n",
        "    vocab['<pad>'] = 0  # add a padding token\n",
        "    vocab['<bos>'] = 1  # begin of sentence\n",
        "    vocab['<eos>'] = 2  # end of sentence\n",
        "\n",
        "    maxlen = -1\n",
        "    for sentence in lines:\n",
        "        for token in sentence:\n",
        "            if token not in vocab:\n",
        "                vocab[token] = index\n",
        "                index += 1\n",
        "\n",
        "        if maxlen < len(sentence):\n",
        "            maxlen = len(sentence)\n",
        "\n",
        "    arr = np.zeros((len(lines), maxlen+2), dtype='int32')\n",
        "\n",
        "    for i, sentence in enumerate(lines):\n",
        "        for j, token in enumerate(sentence):\n",
        "            arr[i, j+1] = vocab[token]\n",
        "        arr[i, 0] = vocab['<bos>']\n",
        "        arr[i, len(sentence)+1] = vocab['<eos>']\n",
        "\n",
        "    return arr, vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BIhmAaaFt69c"
      },
      "outputs": [],
      "source": [
        "x_train, x_vocab = text_encoding(x_tokens)\n",
        "y_train, y_vocab = text_encoding(y_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dM8OY2xxt69c"
      },
      "outputs": [],
      "source": [
        "inverse_x_vocab = {index: token for token, index in x_vocab.items()}\n",
        "inverse_y_vocab = {index: token for token, index in y_vocab.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nExSJaKkt69c"
      },
      "outputs": [],
      "source": [
        "def text_decoding(line, invvocab):\n",
        "    return [ invvocab[x] for x in line]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0DuZliYzt69c",
        "outputId": "74b41990-bc86-48bc-c09e-0dffc5b3d282",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<bos>', '의심', '의', '여', '이', '지', '없이', '세상', '에는', '어떤', '남자', '이', '든', '정확히', '딱', '알맞', '는', '여자', '와', '결혼', '하', '거나', '그', '반대', '의', '상황', '이', '존재', '하', '지', '.', '그런데', '인간', '이', '수백', '명', '의', '사람', '만', '알', '고', '지내', '는', '사이', '가', '되', 'ㄹ', '기회', '를', '갖', '는다', '고', '생각', '하', '어', '보', '면', ',', '또', '그', '수백', '명', '중', '열', '여', '명', '쯤', '이하', '만', '잘', '알', 'ㄹ', '수', '있', '고', ',', '그리고', '나', '서', '그', '열', '여', '명', '중', '에', '한두', '명', '만', '친구', '가', '되', 'ㄹ', '수', '있', '다면', ',', '그리고', '또', '만', '이', 'ㄹ', '우리', '가', '이', '세상', '에', '살', '고', '있', '는', '수백만', '명', '의', '사람들', '만', '기억', '하고', '있', '다면', ',', '딱', '맞', '는', '남자', '는', '지구', '가', '생기', '어', '나', 'ㄴ', '이래', '로', '딱', '맞', '는', '여자', '를', '달', 'ㄴ', '한번', '도', '만나', 'ㄴ', '적', '이', '없', '을', '수', '도', '있', '을', '것', '이', '라는', '사실', '을', '쉽', '게', '눈치채', 'ㄹ', '수', '있', '을', '것', '이', '야', '.', '<eos>']\n"
          ]
        }
      ],
      "source": [
        "print(text_decoding(x_train[-1], inverse_x_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JG80JVx8t69c",
        "outputId": "b57ec19c-f704-4f71-b599-67ac637788da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<bos>', 'doubtless', 'there', 'exists', 'in', 'this', 'world', 'precisely', 'the', 'right', 'woman', 'for', 'any', 'given', 'man', 'to', 'marry', 'and', 'vice', 'versa', ';', 'but', 'when', 'you', 'consider', 'that', 'a', 'human', 'being', 'has', 'the', 'opportunity', 'of', 'being', 'acquainted', 'with', 'only', 'a', 'few', 'hundred', 'people', ',', 'and', 'out', 'of', 'the', 'few', 'hundred', 'that', 'there', 'are', 'but', 'a', 'dozen', 'or', 'less', 'whom', 'he', 'knows', 'intimately', ',', 'and', 'out', 'of', 'the', 'dozen', ',', 'one', 'or', 'two', 'friends', 'at', 'most', ',', 'it', 'will', 'easily', 'be', 'seen', ',', 'when', 'we', 'remember', 'the', 'number', 'of', 'millions', 'who', 'inhabit', 'this', 'world', ',', 'that', 'probably', ',', 'since', 'the', 'earth', 'was', 'created', ',', 'the', 'right', 'man', 'has', 'never', 'yet', 'met', 'the', 'right', 'woman', '.', '<eos>']\n"
          ]
        }
      ],
      "source": [
        "print(text_decoding(y_train[-1], inverse_y_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rX193UHKt69d",
        "outputId": "acf2a2e5-e984-4f9a-9f39-c88bdc0aaaec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2896 2527\n"
          ]
        }
      ],
      "source": [
        "print(len(x_vocab), len(y_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3E4ikUFwt69d",
        "outputId": "f03af305-8d0e-4a02-d202-aa72886d49f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3729, 169)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjUrXFEgt69d"
      },
      "source": [
        "### Save preprocessed dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2mqvvmj6t69d"
      },
      "outputs": [],
      "source": [
        "np.savez('kor-eng', x_train=x_train, y_train=y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "hxspub8Kt69d"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"kor-eng-krvocab.json\", \"w\") as f:\n",
        "    json.dump(x_vocab, f)\n",
        "with open(\"kor-eng-envocab.json\", \"w\") as f:\n",
        "    json.dump(y_vocab, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSYmp4Bgt69d"
      },
      "source": [
        "### Load preprocessed dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ykjI4Iq2t69d"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "npzfile = np.load('kor-eng.npz')\n",
        "x_train = npzfile['x_train']\n",
        "y_train = npzfile['y_train']\n",
        "\n",
        "with open(\"kor-eng-krvocab.json\", \"rb\") as f:\n",
        "    x_vocab = json.load(f)\n",
        "with open(\"kor-eng-envocab.json\", \"rb\") as f:\n",
        "    y_vocab = json.load(f)\n",
        "\n",
        "inverse_x_vocab = {index: token for token, index in x_vocab.items()}\n",
        "inverse_y_vocab = {index: token for token, index in y_vocab.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdAsmj9Bt69d"
      },
      "source": [
        "### Building models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "S-tDMjSMt69d"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(x_train)\n",
        "BATCH_SIZE = 16\n",
        "embedding_dim = 1024\n",
        "latent_dim = 1024\n",
        "x_vocab_size = len(x_vocab)\n",
        "y_vocab_size = len(y_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "25kvNP1it69d"
      },
      "outputs": [],
      "source": [
        "# PyTorch Dataset\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, x_data, y_data):\n",
        "        self.x_data = torch.LongTensor(x_data)\n",
        "        self.y_data = torch.LongTensor(y_data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x_data[idx], self.y_data[idx]\n",
        "\n",
        "dataset = TranslationDataset(x_train, y_train)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "eNbjgMAyt69e",
        "outputId": "2cdcad77-4faa-4bae-c775-f0fbec5d3796",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   1,  470, 2260,  110,  860,   52, 1045, 1100,  144,   11,    2,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0],\n",
            "        [   1, 1110, 1111,  564,   48,  201,  323,   48, 1264, 2394,   48,   40,\n",
            "            5,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0],\n",
            "        [   1,   60,   48,  108,   27,    5,    2,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0],\n",
            "        [   1,   45,  359,  132,  593,   46,  779,   52,  104,    8,  112,   46,\n",
            "          325,  274,    5,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0],\n",
            "        [   1,   58,   59,  110,  157,   27,    5,    2,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0]])\n",
            "\n",
            "tensor([[   1,   81,  485,  159, 1735,  564,  409, 1991,    9,    2,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0],\n",
            "        [   1,  682,  204,  692, 2112,  609, 2113,    4,    2,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0],\n",
            "        [   1,   56,   98,    4,    2,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0],\n",
            "        [   1, 1021,  315,  638,  204,   72,  456,  880,    4,    2,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0],\n",
            "        [   1,   53,  128,    4,    2,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0]])\n"
          ]
        }
      ],
      "source": [
        "for example_input_batch, example_target_batch in dataloader:\n",
        "    print(example_input_batch[:5])\n",
        "    print()\n",
        "    print(example_target_batch[:5])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hMfXkrilt69e"
      },
      "outputs": [],
      "source": [
        "# Compact batch (remove trailing zeros)\n",
        "def compact_batch(batch):\n",
        "    max_len = (batch != 0).sum(dim=1).max().item()\n",
        "    return batch[:, :max_len]\n",
        "\n",
        "example_input_batch = compact_batch(example_input_batch)\n",
        "example_target_batch = compact_batch(example_target_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "g7xlEY0Gt69e",
        "outputId": "18caf912-9a2c-46ce-fa0f-3cb142beab47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   1,  470, 2260,  110,  860,   52, 1045, 1100,  144,   11,    2,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0],\n",
            "        [   1, 1110, 1111,  564,   48,  201,  323,   48, 1264, 2394,   48,   40,\n",
            "            5,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0],\n",
            "        [   1,   60,   48,  108,   27,    5,    2,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0],\n",
            "        [   1,   45,  359,  132,  593,   46,  779,   52,  104,    8,  112,   46,\n",
            "          325,  274,    5,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0],\n",
            "        [   1,   58,   59,  110,  157,   27,    5,    2,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0]])\n"
          ]
        }
      ],
      "source": [
        "print(example_input_batch[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FAKIVPGat69e",
        "outputId": "e1fcecb6-0f0c-48d3-9ff2-0c1592bbb879",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 29])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "example_input_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "b6kRoXKct69e"
      },
      "outputs": [],
      "source": [
        "max_len = (example_input_batch != 0).sum(dim=1).cpu()\n",
        "example_input_batch_packed = nn.utils.rnn.pack_padded_sequence(\n",
        "    example_input_batch, max_len, batch_first=True, enforce_sorted=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "64-LNZE_t69e",
        "outputId": "eca5ca61-d1ec-412b-f9d9-0ecb9c2c888b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([   1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,  783, 2850,   45, 1110,   25,  271,   58,   60,\n",
              "         470, 1688,  162,  643,   58,   60,   58,  466,   72, 2851,  359, 1111,\n",
              "          46, 2119,   59,  910, 2260,  136,   21,  976,   59,   48,   46,   48,\n",
              "          42, 2852,  132,  564,  755, 1467,  110, 1044,  110,   45,  274,   46,\n",
              "         110,  108,  486,  137,   38, 2853,  593,   48,   61, 2292,  138,   46,\n",
              "         860,   46,  134,  126,  157,   27,   94,    5,   48,  218,   46,  201,\n",
              "        1814,  960,  139, 2582,   52, 1689,   21,   48,   27,    5,    5,    2,\n",
              "        1873, 2854,  779,  323,  478,  359,  806,  576, 1045,   48,  633,   40,\n",
              "           5,    2,    2,   21,  422,   52,   48, 1616,  946,  118, 1154, 1100,\n",
              "          99,  144,   11,    2,   52, 1536,  104, 1264,  122,   48,   52,  265,\n",
              "         144,   97,    5,    2,  797, 2855,    8, 2394,  790,  135,   56,  135,\n",
              "          11,    5,    2,  201,  237,  112,   48,   21,    8,   23,    8,    2,\n",
              "           2,  696,   48,   46,   40,    8,    5,    5,    5,  110,  288,  325,\n",
              "           5,    5,    2,    2,    2,   28, 2856,  274,    2,    2,   40,  576,\n",
              "           5,    5, 2771,    2,  963,  484,  110,   18,   48, 1361,   48,   21,\n",
              "         288,   27,   48,    5,  288,    2,  748,    8,  797,   23,    5,    2]), batch_sizes=tensor([16, 16, 16, 16, 16, 16, 15, 13, 12, 11, 10,  8,  8,  5,  3,  3,  2,  2,\n",
              "         2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1]), sorted_indices=tensor([ 6, 13,  3,  1,  9, 11, 12, 15,  0, 10,  7,  5,  4,  2,  8, 14]), unsorted_indices=tensor([ 8,  3, 13,  2, 12, 11,  0, 10, 14,  4,  9,  5,  6,  1, 15,  7]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "example_input_batch_packed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "_W1U1Fu6t69f"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=3):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len)\n",
        "        # Create mask for padding\n",
        "        lengths = (x != 0).sum(dim=1).cpu()\n",
        "\n",
        "        embedded = self.embedding(x)  # (batch, seq_len, embedding_dim)\n",
        "\n",
        "        # Pack sequence for efficient computation\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        packed_out, hidden = self.gru(packed)\n",
        "\n",
        "        # Unpack sequence\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
        "\n",
        "        # hidden: (num_layers, batch, hidden_dim)\n",
        "        # Return each layer's hidden state separately for compatibility\n",
        "        s1 = hidden[0]  # (batch, hidden_dim)\n",
        "        s2 = hidden[1]  # (batch, hidden_dim)\n",
        "        s3 = hidden[2]  # (batch, hidden_dim)\n",
        "\n",
        "        return output, s1, s2, s3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "A38Oe0kHt69f",
        "outputId": "fdb38a60-74e0-4032-a7c9-5f445a993eb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder(\n",
            "  (embedding): Embedding(2896, 1024, padding_idx=0)\n",
            "  (gru): GRU(1024, 1024, num_layers=3, batch_first=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(x_vocab_size, embedding_dim, latent_dim).to(device)\n",
        "print(encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "phHqBJctt69f",
        "outputId": "35ce82fd-c746-457e-9139-ebd5874fbfc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 29, 1024]) torch.Size([16, 1024])\n"
          ]
        }
      ],
      "source": [
        "last_output, last_state1, last_state2, last_state3 = \\\n",
        "    encoder(example_input_batch.to(device))\n",
        "print(last_output.shape, last_state1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "4081CYgLt69f"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=3):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, s1, s2, s3):\n",
        "        # x: (batch, seq_len)\n",
        "        # s1, s2, s3: (batch, hidden_dim)\n",
        "\n",
        "        # Get lengths for packing\n",
        "        lengths = (x != 0).sum(dim=1).cpu()\n",
        "        lengths = lengths.clamp(min=1)  # Ensure minimum length of 1\n",
        "\n",
        "        embedded = self.embedding(x)  # (batch, seq_len, embedding_dim)\n",
        "\n",
        "        # Pack sequence\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # Stack hidden states from all layers: (num_layers, batch, hidden)\n",
        "        hidden = torch.stack([s1, s2, s3], dim=0)\n",
        "\n",
        "        packed_out, out_hidden = self.gru(packed, hidden)\n",
        "\n",
        "        # Unpack sequence\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
        "\n",
        "        logits = self.fc(output)  # (batch, seq_len, vocab_size)\n",
        "\n",
        "        # Return each layer's hidden state separately for compatibility\n",
        "        out_s1 = out_hidden[0]  # (batch, hidden_dim)\n",
        "        out_s2 = out_hidden[1]  # (batch, hidden_dim)\n",
        "        out_s3 = out_hidden[2]  # (batch, hidden_dim)\n",
        "\n",
        "        return logits, out_s1, out_s2, out_s3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "oXP1DYwRt69g",
        "outputId": "81acf935-843b-4a95-ff88-1be7b2cea660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder(\n",
            "  (embedding): Embedding(2527, 1024, padding_idx=0)\n",
            "  (gru): GRU(1024, 1024, num_layers=3, batch_first=True)\n",
            "  (fc): Linear(in_features=1024, out_features=2527, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "decoder = Decoder(y_vocab_size, embedding_dim, latent_dim).to(device)\n",
        "logits, s1, s2, s3 = decoder(example_target_batch.to(device), \\\n",
        "                             last_state1, last_state2, last_state3)\n",
        "print(decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lDsM78hXt69g",
        "outputId": "99baf4b2-1138-44bd-fe76-1e0637b97573",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 18, 2527]) torch.Size([16, 1024]) torch.Size([16, 1024]) torch.Size([16, 1024])\n"
          ]
        }
      ],
      "source": [
        "print(logits.shape, s1.shape, s2.shape, s3.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPZ6i93Jt69g"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "o4uUYaZwt69g"
      },
      "outputs": [],
      "source": [
        "def batch_loss(y_true, y_pred):\n",
        "    # y_true: (batch, seq_len)\n",
        "    # y_pred: (batch, seq_len, vocab_size)\n",
        "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    # Reshape for CrossEntropyLoss: (batch * seq_len, vocab_size)\n",
        "    batch_size, seq_len, vocab_size = y_pred.shape\n",
        "    y_pred_flat = y_pred.reshape(-1, vocab_size)\n",
        "    y_true_flat = y_true.reshape(-1)\n",
        "\n",
        "    loss = loss_fn(y_pred_flat, y_true_flat)\n",
        "    loss = loss.reshape(batch_size, seq_len)\n",
        "\n",
        "    # Mask padding tokens\n",
        "    mask = (y_true != 0).float()\n",
        "    loss = loss * mask\n",
        "\n",
        "    return loss.sum() / mask.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "526ugNHZt69g",
        "outputId": "d53a75f4-515f-45cd-d933-62f9e3e28611",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.8361, grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "batch_loss(example_target_batch[:, 1:].to(device), logits[:, :-1, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHM6fd01t69g"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "B2ln9jOVt69g"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ySBbSAKLt69g"
      },
      "outputs": [],
      "source": [
        "def predict(x_batch, y_batch, training=True):\n",
        "    encoder.train(training)\n",
        "    decoder.train(training)\n",
        "\n",
        "    _, s1, s2, s3 = encoder(x_batch)\n",
        "    logits, _, _, _ = decoder(y_batch, s1, s2, s3)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "3Ht0w5mCt69g"
      },
      "outputs": [],
      "source": [
        "def train_step(x_batch, y_batch):\n",
        "    # Compact batch tensors\n",
        "    x_batch = compact_batch(x_batch).to(device)\n",
        "    y_batch = compact_batch(y_batch).to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Encoder & decoder\n",
        "    logits = predict(x_batch, y_batch, training=True)\n",
        "\n",
        "    # Loss: compare y_batch[:, 1:] with logits[:, :-1, :]\n",
        "    loss = batch_loss(y_batch[:, 1:], logits[:, :-1, :])\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS8pyMsAt69g",
        "outputId": "ea619a63-f776-438e-a24b-1366f3bf8d70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for epoch 1 is 476.55 sec: training loss = 4.098871\n",
            "Time for epoch 2 is 468.05 sec: training loss = 7.214556\n",
            "Time for epoch 3 is 464.21 sec: training loss = 3.637597\n",
            "Time for epoch 4 is 466.61 sec: training loss = 2.035397\n",
            "Time for epoch 5 is 462.48 sec: training loss = 1.346120\n",
            "Time for epoch 6 is 458.19 sec: training loss = 3.879420\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(50):\n",
        "    start = time.time()\n",
        "\n",
        "    loss_sum = 0\n",
        "    for x_batch, y_batch in dataloader:\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "        loss_sum += loss\n",
        "\n",
        "    print('Time for epoch {} is {:.2f} sec: training loss = {:.6f}'.format(\n",
        "        epoch + 1, time.time() - start, loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHOjG7KAt69g"
      },
      "outputs": [],
      "source": [
        "# Save model weights\n",
        "torch.save(encoder.state_dict(), 'nmt-wo-attention.encoder.pt')\n",
        "torch.save(decoder.state_dict(), 'nmt-wo-attention.decoder.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q_KkDi6t69h"
      },
      "source": [
        "### Test translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-H2NUBEt69h"
      },
      "outputs": [],
      "source": [
        "def translate(src, max_steps=100):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    # Tokenization\n",
        "    src_tokens = np.array([x_vocab['<bos>']] + [x_vocab[x] for x in morph.morphs(src)] + [x_vocab['<eos>']])\n",
        "\n",
        "    print([inverse_x_vocab[x] for x in src_tokens])\n",
        "\n",
        "    # Add the batch axis\n",
        "    x_test = torch.LongTensor(src_tokens).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Compute encoder and get hidden states\n",
        "        _, s1, s2, s3 = encoder(x_test)\n",
        "\n",
        "        # y_test: add the batch axis\n",
        "        y_test = torch.LongTensor([[y_vocab['<bos>']]]).to(device)\n",
        "        output_seq = []\n",
        "\n",
        "        for _ in range(max_steps):\n",
        "            logits, s1, s2, s3 = decoder(y_test, s1, s2, s3)\n",
        "\n",
        "            # Greedily use the token with the highest logit\n",
        "            y_test = logits.argmax(dim=2)\n",
        "            pred = y_test.squeeze(0).item()\n",
        "\n",
        "            # If prediction is eos, output sequence is complete\n",
        "            if pred == y_vocab['<eos>']:\n",
        "                break\n",
        "            output_seq.append(pred)\n",
        "\n",
        "    return ' '.join([inverse_y_vocab[x] for x in output_seq])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbuRXJbTt69h"
      },
      "outputs": [],
      "source": [
        "translate('잘 안된다.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2oUhJlzt69h"
      },
      "outputs": [],
      "source": [
        "def beam_translate(src, max_steps=100, k=16):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    # Tokenization\n",
        "    src_tokens = np.array([x_vocab['<bos>']] + [x_vocab[x] for x in morph.morphs(src)] + [x_vocab['<eos>']])\n",
        "    print(morph.morphs(src))\n",
        "\n",
        "    # Add the batch axis\n",
        "    x_test = torch.LongTensor(src_tokens).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Compute encoder and get hidden states\n",
        "        _, s1, s2, s3 = encoder(x_test)\n",
        "\n",
        "        # Init candidates: (score, last_token, s1, s2, s3, output_seq, eos)\n",
        "        last_token = torch.LongTensor([[y_vocab['<bos>']]]).to(device)\n",
        "        candidates = [(0., last_token, s1, s2, s3, [y_vocab['<bos>']], False)]\n",
        "\n",
        "        for _ in range(max_steps):\n",
        "            new_candidates = []\n",
        "\n",
        "            for score, token, c_s1, c_s2, c_s3, output_seq, eos in candidates:\n",
        "                # If the candidate already ends\n",
        "                if eos:\n",
        "                    new_candidates.append((score, token, c_s1, c_s2, c_s3, output_seq, eos))\n",
        "                    continue\n",
        "\n",
        "                # Compute the prob. of following tokens\n",
        "                logits, new_s1, new_s2, new_s3 = decoder(token, c_s1, c_s2, c_s3)\n",
        "                # shape of logits: (1, 1, vocab_size)\n",
        "                probs = torch.log_softmax(logits, dim=2)\n",
        "\n",
        "                # Use the token with the top-k logits\n",
        "                values, indices = torch.topk(probs.squeeze(), k=k)\n",
        "\n",
        "                for prob, idx in zip(values, indices):\n",
        "                    idx_val = idx.item()\n",
        "                    # If prediction is eos, output sequence is complete\n",
        "                    is_eos = (idx_val == y_vocab['<eos>'])\n",
        "\n",
        "                    new_token = torch.LongTensor([[idx_val]]).to(device)\n",
        "                    new_candidates.append(\n",
        "                        (score + prob.item(), new_token, new_s1, new_s2, new_s3,\n",
        "                         output_seq + [idx_val], is_eos)\n",
        "                    )\n",
        "\n",
        "            candidates = sorted(new_candidates, key=lambda t: -t[0])[:k]\n",
        "\n",
        "    return [(candidates[i][0], ' '.join([inverse_y_vocab[x] for x in candidates[i][5]])) for i in range(k)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uexAogZ3t69h"
      },
      "outputs": [],
      "source": [
        "beam_translate('잘 안된다.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-gqmzWEuERH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}